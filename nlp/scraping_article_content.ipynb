{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "collections = db_client['db_migration']\n",
    "tb_article = collections['tb_article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_URL = 'https://thenewhumanitarian.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_data(url):\n",
    "    try:\n",
    "        # Set up Chrome WebDriver\n",
    "        service = Service('../chromedriver') # or geckodriver for firefox\n",
    "        options = webdriver.ChromeOptions()\n",
    "        #options.add_argument('headless')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"--window-size=0,0\")\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        driver = webdriver.Chrome(service=service, options=options) # or webdriver.Firefox(service=service)\n",
    "\n",
    "        # Navigate to the webpage\n",
    "        driver.get(url)\n",
    "        # driver.minimize_window()\n",
    "        # driver.set_window_size(1, 1)\n",
    "\n",
    "        # Wait for JavaScript to load (adjust time as needed)\n",
    "        driver.implicitly_wait(10)  # Waits up to 10 seconds for elements to appear\n",
    "\n",
    "        # Get the rendered HTML\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        return soup\n",
    "    except:\n",
    "        print('Error when scraping webpage')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_article_meta(article_detail):\n",
    "        db_article = tb_article.find_one({'url': article_detail['url']})\n",
    "        if db_article is None:\n",
    "            #insert\n",
    "            tb_article.insert_one(article_detail)\n",
    "            # print(\"Inserted +++++++++++ article: \" + article_detail['title'])\n",
    "        else:\n",
    "            #update\n",
    "            tb_article.update_one({'url': article_detail['url']}, {'$set': article_detail})\n",
    "            # print(\"Updated ............ article: \" + article_detail['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There may have different content type\n",
    "#1: normal article: (https://www.thenewhumanitarian.org/opinion/2025/02/04/how-europe-can-escape-migration-deterrence-trap)\n",
    "#2: report article: (https://www.thenewhumanitarian.org/analysis/2025/01/07/trends-will-spur-humanitarian-needs-2025)\n",
    "def extract_content(soup):\n",
    "    content = ''\n",
    "    #type 1\n",
    "    big_content = soup.find('div', attrs={'class': 'field-name-body flow'})\n",
    "    if big_content is not None:\n",
    "        items = big_content.find_all('p')\n",
    "\n",
    "        no_of_paragraphs = len(items)\n",
    "        #print(str(no_of_paragraphs))\n",
    "        for i in range(0, no_of_paragraphs):\n",
    "            if i == no_of_paragraphs -1:\n",
    "                #this is the last item, we need to check whether it is note or not (https://www.thenewhumanitarian.org/opinion/2025/02/04/how-europe-can-escape-migration-deterrence-trap)\n",
    "                em_tag = items[i].find('em')\n",
    "                if em_tag is not None:\n",
    "                    break   #do not include this paragraph in the final content\n",
    "            content += items[i].text.strip()\n",
    "    #type 2\n",
    "    items = soup.find_all('div', attrs={'class': 'advanced-report-content flow'})\n",
    "    no_of_paragraphs = len(items)\n",
    "    #print(str(no_of_paragraphs))\n",
    "    for i in range(0, no_of_paragraphs):\n",
    "        p = items[i].find_all('p')\n",
    "        for j in range(0, len(p)):\n",
    "            content += p[j].text.strip()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish page: 1\n",
      "Finish page: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m#delay for 1 second\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinish page: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(index))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#find article that haven't scraped its content\n",
    "db_articles = tb_article.find({'is_scraped': 0})\n",
    "index = 0\n",
    "if db_articles is not None:\n",
    "    for db_article in db_articles:\n",
    "        if index < 11000:\n",
    "            url = db_article['url']\n",
    "            soup = scrape_text_data(DOMAIN_URL + url)\n",
    "            if soup == '':\n",
    "                print('Error no soup: ' + url)\n",
    "                db_article['error'] = 'no soup'\n",
    "                tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "            else:\n",
    "                db_article['is_scraped'] = 1\n",
    "                content = extract_content(soup)\n",
    "                if content == '':\n",
    "                    db_article['error'] = 'no content'\n",
    "                    tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "                    #print('Error content in page: ' + url)\n",
    "                else:\n",
    "                    #correct data\n",
    "                    db_article['error'] = ''\n",
    "                    db_article['content'] = content\n",
    "                    db_article['len_content'] = len(content)\n",
    "                    tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "                    # print(content)\n",
    "                    #print('Updated content: ' + url)\n",
    "        else:\n",
    "            break\n",
    "        index += 1\n",
    "        time.sleep(1)   #delay for 1 second\n",
    "        print('Finish page: ' + str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
