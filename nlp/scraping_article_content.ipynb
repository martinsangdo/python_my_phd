{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "collections = db_client['db_migration']\n",
    "tb_article = collections['tb_article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_URL = 'https://thenewhumanitarian.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_data(url):\n",
    "    try:\n",
    "        # Set up Chrome WebDriver\n",
    "        service = Service('../chromedriver') # or geckodriver for firefox\n",
    "        options = webdriver.ChromeOptions()\n",
    "        #options.add_argument('headless')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"--window-size=0,0\")\n",
    "        driver = webdriver.Chrome(service=service, options=options) # or webdriver.Firefox(service=service)\n",
    "\n",
    "        # Navigate to the webpage\n",
    "        driver.get(url)\n",
    "        # driver.minimize_window()\n",
    "        # driver.set_window_size(1, 1)\n",
    "\n",
    "        # Wait for JavaScript to load (adjust time as needed)\n",
    "        driver.implicitly_wait(10)  # Waits up to 10 seconds for elements to appear\n",
    "\n",
    "        # Get the rendered HTML\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        return soup\n",
    "    except:\n",
    "        print('Error when scraping webpage')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_article_meta(article_detail):\n",
    "        db_article = tb_article.find_one({'url': article_detail['url']})\n",
    "        if db_article is None:\n",
    "            #insert\n",
    "            tb_article.insert_one(article_detail)\n",
    "            # print(\"Inserted +++++++++++ article: \" + article_detail['title'])\n",
    "        else:\n",
    "            #update\n",
    "            tb_article.update_one({'url': article_detail['url']}, {'$set': article_detail})\n",
    "            # print(\"Updated ............ article: \" + article_detail['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There may have different content type\n",
    "#1: normal article: (https://www.thenewhumanitarian.org/opinion/2025/02/04/how-europe-can-escape-migration-deterrence-trap)\n",
    "#2: report article: (https://www.thenewhumanitarian.org/analysis/2025/01/07/trends-will-spur-humanitarian-needs-2025)\n",
    "def extract_content(soup):\n",
    "    content = ''\n",
    "    #type 1\n",
    "    big_content = soup.find('div', attrs={'class': 'field-name-body flow'})\n",
    "    items = big_content.find_all('p')\n",
    "\n",
    "    no_of_paragraphs = len(items)\n",
    "    #print(str(no_of_paragraphs))\n",
    "    for i in range(0, no_of_paragraphs):\n",
    "        if i == no_of_paragraphs -1:\n",
    "            #this is the last item, we need to check whether it is note or not (https://www.thenewhumanitarian.org/opinion/2025/02/04/how-europe-can-escape-migration-deterrence-trap)\n",
    "            em_tag = items[i].find('em')\n",
    "            if em_tag is not None:\n",
    "                break   #do not include this paragraph in the final content\n",
    "        content += items[i].text.strip()\n",
    "    #type 2\n",
    "    items = soup.find_all('div', attrs={'class': 'advanced-report-content flow'})\n",
    "    no_of_paragraphs = len(items)\n",
    "    #print(str(no_of_paragraphs))\n",
    "    for i in range(0, no_of_paragraphs):\n",
    "        p = items[i].find_all('p')\n",
    "        for j in range(0, len(p)):\n",
    "            content += p[j].text.strip()\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated content: /interview/2024/12/05/us-americas-what-are-safety-mobility-offices-and-will-trumps-migrant-crackdown-scupper\n"
     ]
    }
   ],
   "source": [
    "#find article that haven't scraped its content\n",
    "db_article = tb_article.find_one({'is_scraped': 0})\n",
    "if db_article is not None:\n",
    "    url = db_article['url']\n",
    "    soup = scrape_text_data(DOMAIN_URL + url)\n",
    "    if soup == '':\n",
    "        print('Error no soup: ' + url)\n",
    "        db_article['error'] = 'no soup'\n",
    "        tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "    else:\n",
    "        db_article['is_scraped'] = 1\n",
    "        content = extract_content(soup)\n",
    "        if content == '':\n",
    "            db_article['error'] = 'no content'\n",
    "            tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "            print('Error content in page: ' + url)\n",
    "        else:\n",
    "            #correct data\n",
    "            db_article['error'] = ''\n",
    "            db_article['content'] = content\n",
    "            db_article['len_content'] = len(content)\n",
    "            tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "            # print(content)\n",
    "            print('Updated content: ' + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
