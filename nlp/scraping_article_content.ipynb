{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "collections = db_client['db_migration']\n",
    "tb_article = collections['tb_article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_URL = 'https://thenewhumanitarian.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_text_data(url):\n",
    "    try:\n",
    "        # Set up Chrome WebDriver\n",
    "        service = Service('../chromedriver') # or geckodriver for firefox\n",
    "        options = webdriver.ChromeOptions()\n",
    "        #options.add_argument('headless')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"--window-size=0,0\")\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        driver = webdriver.Chrome(service=service, options=options) # or webdriver.Firefox(service=service)\n",
    "\n",
    "        # Navigate to the webpage\n",
    "        driver.get(url)\n",
    "        # driver.minimize_window()\n",
    "        # driver.set_window_size(1, 1)\n",
    "\n",
    "        # Wait for JavaScript to load (adjust time as needed)\n",
    "        driver.implicitly_wait(10)  # Waits up to 10 seconds for elements to appear\n",
    "\n",
    "        # Get the rendered HTML\n",
    "        html = driver.page_source\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        return soup\n",
    "    except:\n",
    "        print('Error when scraping webpage')\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_article_meta(article_detail):\n",
    "        db_article = tb_article.find_one({'url': article_detail['url']})\n",
    "        if db_article is None:\n",
    "            #insert\n",
    "            tb_article.insert_one(article_detail)\n",
    "            # print(\"Inserted +++++++++++ article: \" + article_detail['title'])\n",
    "        else:\n",
    "            #update\n",
    "            tb_article.update_one({'url': article_detail['url']}, {'$set': article_detail})\n",
    "            # print(\"Updated ............ article: \" + article_detail['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text in all tags <p>\n",
    "def get_content_in_container(container):\n",
    "    content = ''\n",
    "    items = container.find_all('p')\n",
    "    no_of_paragraphs = len(items)\n",
    "    # print(str(no_of_paragraphs))\n",
    "    if no_of_paragraphs == 0:\n",
    "        items = container.find_all('div')\n",
    "        no_of_paragraphs = len(items)\n",
    "        # print(str(no_of_paragraphs))\n",
    "        if no_of_paragraphs == 0:\n",
    "            items = container.find_all('span')\n",
    "            no_of_paragraphs = len(items)\n",
    "            # print(str(no_of_paragraphs))\n",
    "    for i in range(0, no_of_paragraphs):\n",
    "        if i == no_of_paragraphs - 1:\n",
    "            #this is the last item, we need to check whether it is note or not (https://www.thenewhumanitarian.org/opinion/2025/02/04/how-europe-can-escape-migration-deterrence-trap)\n",
    "            em_tag = items[i].find('em')\n",
    "            if em_tag is not None:\n",
    "                break   #do not include this paragraph in the final content\n",
    "        if ('Our ability to deliver compelling' not in items[i].text and\n",
    "            'Your donation will support our unique approach to journalism' not in items[i].text and\n",
    "            'joining our membership programme' not in items[i].text):   #remove footnote\n",
    "            content += items[i].text.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There may have different content type\n",
    "#1: normal article: (https://www.thenewhumanitarian.org/opinion/2025/02/04/how-europe-can-escape-migration-deterrence-trap)\n",
    "#2: report article: (https://www.thenewhumanitarian.org/analysis/2025/01/07/trends-will-spur-humanitarian-needs-2025)\n",
    "#3: stories: https://interactive.thenewhumanitarian.org/stories/2022/05/10/us-asylum-darien-gap-cuba-central-america-mexico/\n",
    "#4: text in span: https://www.thenewhumanitarian.org/feature/2017/04/26/preying-disaster-how-human-trafficking-has-spiked-quake-shattered-nepal\n",
    "#5: text in div: https://www.thenewhumanitarian.org/news/2017/04/13/pushed-out-pakistan-war-torn-afghanistan-refugees-are-told-be-patient\n",
    "def extract_content(soup):\n",
    "    content = ''\n",
    "    #type 1 & 4\n",
    "    article_body = soup.find('div', attrs={'class': 'article__body'})\n",
    "    if article_body is not None:\n",
    "        big_contents = article_body.find_all('div', attrs={'class': 'field-name-body flow'})\n",
    "        for big_content in big_contents:\n",
    "            content += get_content_in_container(big_content)\n",
    "            # print('============')\n",
    "    #type 2\n",
    "    items = soup.find_all('div', attrs={'class': 'advanced-report-content flow'})\n",
    "    no_of_paragraphs = len(items)\n",
    "    #print(str(no_of_paragraphs))\n",
    "    for i in range(0, no_of_paragraphs):\n",
    "        p = items[i].find_all('p')\n",
    "        for j in range(0, len(p)):\n",
    "            content += p[j].text.strip()\n",
    "    #type 3\n",
    "    field_name_bodies = soup.find_all('div', attrs={'class': 'field-name-body'})\n",
    "    if field_name_bodies is not None:\n",
    "        # print(str(len(field_name_bodies)))\n",
    "        for field_name_body in field_name_bodies:\n",
    "            content += get_content_in_container(field_name_body)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "============\n",
      "0\n",
      "62\n",
      "============\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "62\n",
      "3\n",
      "Finish page: 1 https://thenewhumanitarian.org/news/2017/04/13/pushed-out-pakistan-war-torn-afghanistan-refugees-are-told-be-patient\n",
      "Error when scraping webpage\n",
      "Error no soup: https://thenewhumanitarian.org/feature/2017/04/11/techno-utopian-solutions-syria-s-refugee-crisis-fall-short\n"
     ]
    }
   ],
   "source": [
    "#find article that haven't scraped its content\n",
    "db_articles = tb_article.find({'len_content': 0})\n",
    "index = 0\n",
    "if db_articles is not None:\n",
    "    for db_article in db_articles:\n",
    "        if index < 11000:\n",
    "            url = db_article['url']\n",
    "            soup = scrape_text_data(DOMAIN_URL + url)\n",
    "            if soup == '':\n",
    "                print('Error no soup: ' + DOMAIN_URL + url)\n",
    "                db_article['error'] = 'no soup'\n",
    "                tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "                break\n",
    "            else:\n",
    "                db_article['is_scraped'] = 1\n",
    "                content = extract_content(soup)\n",
    "                if content == '':\n",
    "                    db_article['error'] = 'no content'\n",
    "                    tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "                    print('No content in page: ' + DOMAIN_URL + url)\n",
    "                    break\n",
    "                else:\n",
    "                    #correct data\n",
    "                    db_article['error'] = ''\n",
    "                    db_article['content'] = content\n",
    "                    db_article['len_content'] = len(content)\n",
    "                    tb_article.update_one({'url': url}, {'$set': db_article})\n",
    "                    #print('Updated content: ' + url)\n",
    "        else:\n",
    "            break\n",
    "        index += 1\n",
    "        time.sleep(1)   #delay for 1 second\n",
    "        print('Finish page: ' + str(index) + ' ' + DOMAIN_URL + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
